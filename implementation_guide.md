ğŸ— Sentinel AI: Monorepo Structure & Implementation GuideSince "Sentinel AI" is a multi-service product (Node.js, Python, Frontend, CLI), the best approach is a Monorepo. This allows you to share Types (TypeScript interfaces) between your Backend and Frontend, and manage all infrastructure in one place.ğŸ“‚ Recommended Directory StructureWe will use a structure compatible with tools like Turborepo or Nx, but it works fine with standard npm workspaces too.sentinel-ai/
â”œâ”€â”€ ğŸ“‚ apps/
â”‚   â”œâ”€â”€ ğŸ“‚ web/                 # (Next.js 14) - The Dashboard
â”‚   â”‚   â”œâ”€â”€ app/                # App Router
â”‚   â”‚   â”œâ”€â”€ components/         # React Components (Mermaid, Chat)
â”‚   â”‚   â””â”€â”€ lib/                # API Clients
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ api/                 # (NestJS) - The Orchestrator
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ auth/           # Passport Strategies (GitHub, JWT)
â”‚   â”‚   â”‚   â”œâ”€â”€ github/         # Probot / Webhook Handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ repo/           # Repo Management Logic
â”‚   â”‚   â”‚   â””â”€â”€ queue/          # BullMQ Job Processors
â”‚   â”‚   â””â”€â”€ prisma/             # Database Schema
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ ai-engine/           # (Python FastAPI) - The Brain
â”‚       â”œâ”€â”€ app/
â”‚       â”‚   â”œâ”€â”€ core/           # LLM Logic & Prompts
â”‚       â”‚   â”œâ”€â”€ rag/            # Vector Embeddings & Retrieval
â”‚       â”‚   â””â”€â”€ redteam/        # Exploit Generation Logic
â”‚       â””â”€â”€ main.py             # Entry point
â”‚
â”œâ”€â”€ ğŸ“‚ packages/
â”‚   â”œâ”€â”€ ğŸ“‚ database/            # Shared Prisma Client (Optional, or keep in api)
â”‚   â”œâ”€â”€ ğŸ“‚ shared-types/        # Shared TS Interfaces (e.g., ReviewResult)
â”‚   â””â”€â”€ ğŸ“‚ cli/                 # (Node.js) - The 'sentinel' command line tool
â”‚       â”œâ”€â”€ bin/
â”‚       â””â”€â”€ src/
â”‚
â”œâ”€â”€ ğŸ“‚ infra/                   # Infrastructure as Code
â”‚   â”œâ”€â”€ docker-compose.yml      # Postgres (pgvector), Redis
â”‚   â””â”€â”€ k8s/                    # Kubernetes configs (future)
â”‚
â”œâ”€â”€ .env                        # Root environment variables
â””â”€â”€ README.md
ğŸ—„ï¸ Core Database Schema (PostgreSQL)You need to store Users, Repositories, and the Vector Embeddings for the RAG system.// schema.prisma

model User {
  id            String   @id @default(uuid())
  email         String   @unique
  githubId      String?  @unique
  avatarUrl     String?
  apiKeys       ApiKey[]
  repositories  Repository[]
  reviews       Review[]
}

model Repository {
  id            String   @id @default(uuid())
  githubRepoId  Int      @unique
  name          String   // e.g. "facebook/react"
  ownerId       String
  owner         User     @relation(fields: [ownerId], references: [id])
  isIndexed     Boolean  @default(false) // Has RAG indexing finished?
  reviews       Review[]
}

model Review {
  id            String   @id @default(uuid())
  repoId        String
  repository    Repository @relation(fields: [repoId], references: [id])
  prNumber      Int
  commitHash    String
  
  // The JSON output from the AI
  score         Int
  summary       String
  securityIssues Json
  
  createdAt     DateTime @default(now())
}

// Note: Vector embeddings are usually stored in a separate table managed 
// by python/pgvector, or a specialized field type if supported.
ğŸ“… Phase 1: The "Walking Skeleton" (Sprint Plan)To get to the "Product Version" efficiently, focus on this order of operations:Step 1: The Infrastructure (Day 1)Create the docker-compose.yml with PostgreSQL (ensure pgvector image is used) and Redis.Verify you can connect to the DB from both NestJS and Python.Step 2: The GitHub Connection (Day 2-3)NestJS: Implement the GitHub App Webhook listener.Goal: When you open a PR in a dummy repo, your local NestJS server logs: "Received PR #1 event from user X".Goal: Make NestJS fetch the diff URL from GitHub.Step 3: The AI Brain (Day 4-5)Python: Create a simplified endpoint /analyze-diff.Input: Raw Diff text.Output: Hardcoded JSON response (mocking the LLM).Integration: Connect NestJS to call this Python endpoint.Step 4: The Loop (Day 6)NestJS: Take the JSON response from Python and post it back to GitHub using the GitHub API (POST /repos/:owner/:repo/issues/:number/comments).Milestone: You open a PR, and 10 seconds later, your bot comments "Hello World".Step 5: Intelligence (Week 2)Replace the mock Python response with real LangChain/OpenAI calls.Implement the Vector Store ingestion (RAG).